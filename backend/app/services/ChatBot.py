# -*- coding: utf-8 -*-
"""pfsChatBotFinal.ipynb

Automatically generated by Colab.
"""

import ast
import os
import traceback
import logging
from flask_jwt_extended import get_jwt_identity
from flask_sqlalchemy import SQLAlchemy
from langchain.schema import Document
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_core.tools import tool
from langchain_core.messages import SystemMessage, AIMessage, HumanMessage
from langgraph.graph import StateGraph, END
from langgraph.checkpoint.memory import MemorySaver
from langgraph.graph import MessagesState
from langgraph.prebuilt import ToolNode, tools_condition
from langchain.chat_models import init_chat_model
from pydantic import BaseModel, Field
from typing import List, Optional
from app import db
from app.model.Book import Book
from app.model.Article import Article

# Set up logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Pydantic models for structured output
class BookRecommendation(BaseModel):
    book_id: int = Field(description="Unique ID of the book")
    title: str = Field(description="Title of the recommended book")
    authors: List[str] = Field(description="Authors of the book")
    categories: List[str] = Field(description="Categories of the book")
    description: str = Field(description="Description of the book")
    summary: str = Field(description="Summary of the book") 
    rating: float = Field(description="Rating of the book")
    borrow_count: int = Field(description="Number of times the book has been borrowed")
    total_books: int = Field(description="Total number of copies")
    available_books: int = Field(description="Number of available copies")
    featured_book: bool = Field(description="Whether the book is featured")
    cover_url: Optional[str] = Field(default=None, description="URL of the book cover")

class ArticleRecommendation(BaseModel):
    id: str = Field(description="The unique identifier of the article")
    slug: str = Field(description="The unique slug of the article")
    title: str = Field(description="Title of the article")
    author: str = Field(description="Author of the article")
    category: str = Field(description="Category of the article")
    summary: str = Field(description="Summary of the article")
    pdf_url: str = Field(description="URL of the article PDF")
    cover_image_url: Optional[str] = Field(default=None, description="URL of the article cover image")
    read_time: int = Field(description="Estimated read time in minutes")
    views: int = Field(description="Number of views")
    likes: int = Field(description="Number of likes")
    

class ChatResponse(BaseModel):
    answer: str = Field(description="The chatbot's response to the user's query")
    follow_up_question: str = Field(description="A question to continue the conversation")
    recommended_books: Optional[List[BookRecommendation]] = Field(
        default=None,
        description="List of recommended books based on the query"
    )
    recommended_articles: Optional[List[ArticleRecommendation]] = Field(
        default=None,
        description="List of recommended articles based on the query"
    )

class ChatBot:
    private_instance = None
    _initialized = False

    def __new__(cls, app=None):
        if cls.private_instance is None:
            cls.private_instance = super(ChatBot, cls).__new__(cls)
        return cls.private_instance

    def __init__(self, app=None):
        if self._initialized:
            return
        self.app = app
        self.llm = init_chat_model("gpt-4o-mini", model_provider="openai")
        self.embeddings = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")
        self.vector_store = self.load_content_from_db()
        self.graph = self.create_graph()
        self._initialized = True

    def book_to_document(self, book):
        """Convert a SQLAlchemy Book row to a LangChain Document."""
        authors_text = ", ".join([author.name for author in book.authors]) if book.authors else "Unknown"
        categories_text = ", ".join([category.name for category in book.categories]) if book.categories else "Unknown"
        content = f"""
        Type: Book
        Book ID: {book.id or 0}
        Title: {book.title or 'Unknown'}
        Author: {authors_text}
        Category: {categories_text}
        Description: {book.description or 'No description'}
        Summary: {book.summary or 'No summary'}
        Rating: {book.rating or 0}
        Borrow Count: {book.borrow_count or 0}
        Total Books: {book.total_books or 0}
        Available Books: {book.available_books or 0}
        Featured Book: {book.featured_book or False}
        Cover URL: {book.cover_url or ''}
        """
        return Document(
            page_content=content.strip(),
            metadata={"title": book.title or "Unknown", "id": f"book_{book.id}", "type": "book"}
        )

    def article_to_document(self, article):
        """Convert a SQLAlchemy Article row to a LangChain Document."""
        author_name = article.author.name if article.author else "Unknown"
        
        # First debug the actual database values
        logger.debug(f"Database Article {article.id}: pdf_url='{article.pdf_url}', cover_image_url='{article.cover_image_url}'")
        
        content = f"""
        Type: Article
        Article ID: {article.id or 0}
        Slug: {article.slug or 'unknown'}
        Title: {article.title or 'Unknown'}
        Author: {author_name}
        Category: {article.category or 'Unknown'}
        Summary: {article.summary or 'No summary'}
        PDF URL: {article.pdf_url or ''}
        Cover Image URL: {article.cover_image_url or 'https://placehold.co/600x300'}
        Read Time: {article.meta.read_time if article.meta else 5}
        Views: {article.meta.views if article.meta else 0}
        Likes: {article.meta.likes_count if article.meta else 0}
        """
        return Document(
            page_content=content.strip(),
            metadata={"title": article.title or "Unknown", "id": f"article_{article.id}", "type": "article"}
        )






    def add_book_to_vector_store(self, book_id: int):
        """Add a book to the vector store by ID."""
        with self.app.app_context():
            row = db.session.query(Book).get(book_id)
            if row:
                doc = self.item_to_document(row, "book")
                self.vector_store.add_documents([doc])
                logger.info(f"Added book {book_id} to vector store.")

    def add_article_to_vector_store(self, article_id: int):
        """Add an article to the vector store by ID."""
        with self.app.app_context():
            row = db.session.query(Article).get(article_id)
            if row:
                doc = self.item_to_document(row, "article")
                self.vector_store.add_documents([doc])
                logger.info(f"Added article {article_id} to vector store.")

    def remove_book_from_vector_store(self, book_id: int):
        """Remove a book from the vector store by ID."""
        self.vector_store.delete([f"book_{book_id}"])
        logger.info(f"Removed book {book_id} from vector store.")

    def remove_article_from_vector_store(self, article_id: int):
        """Remove an article from the vector store by ID."""
        self.vector_store.delete([f"article_{article_id}"])
        logger.info(f"Removed article {article_id} from vector store.")

    def update_book_in_vector_store(self, book_id: int):
        """Update a book in the vector store by ID."""
        with self.app.app_context():
            row = db.session.query(Book).get(book_id)
            if row:
                self.vector_store.delete([f"book_{book_id}"])
                doc = self.book_to_document(row)
                self.vector_store.add_documents([doc])
                logger.info(f"Updated book {book_id} in vector store.")

    def add_article_to_vector_store(self, article_id: int):
        """Add an article to the vector store by ID."""
        with self.app.app_context():
            row = db.session.query(Article).get(article_id)
            if row:
                doc = self.article_to_document(row)
                self.vector_store.add_documents([doc])
                logger.info(f"Added article {article_id} to vector store.")

    def remove_article_from_vector_store(self, article_id: int):
        """Remove an article from the vector store by ID."""
        self.vector_store.delete([f"article_{article_id}"])
        logger.info(f"Removed article {article_id} from vector store.")

    def update_article_in_vector_store(self, article_id: int):
        """Update an article in the vector store by ID."""
        with self.app.app_context():
            row = db.session.query(Article).get(article_id)
            if row:
                self.vector_store.delete([f"article_{article_id}"])
                doc = self.article_to_document(row)
                self.vector_store.add_documents([doc])
                logger.info(f"Updated article {article_id} in vector store.")

    def load_content_from_db(self):
        """Load books and articles from the database into the FAISS vector store."""
        cache_path = "./content_vectorstore"
        cache_index = f"{cache_path}/index.faiss"
        cache_metadata = f"{cache_path}/index.pkl"

        if os.path.exists(cache_index) and os.path.exists(cache_metadata):
            try:
                logger.info("Loading cached FAISS vector store...")
                vector_store = FAISS.load_local(
                    folder_path=cache_path,
                    embeddings=self.embeddings,
                    allow_dangerous_deserialization=True
                )
                logger.info("Cached vector store loaded successfully.")
                return vector_store
            except Exception as e:
                logger.error(f"Failed to load cached vector store: {e}", exc_info=True)

        logger.info("Building new FAISS vector store from database...")
        with self.app.app_context():
            try:
                books = db.session.query(Book).limit(200).all()
                articles = db.session.query(Article).limit(200).all()
                if not books and not articles:
                    logger.warning("No content found in database. Creating empty vector store.")
                    return FAISS.from_texts(["placeholder empty library"], embedding=self.embeddings)
            except Exception as e:
                logger.error(f"Failed to query content from database: {e}")
                return FAISS.from_texts(["Error accessing library database"], embedding=self.embeddings)

            docs = []
            for book in books:
                doc = self.book_to_document(book)
                docs.append(doc)
            for article in articles:
                doc = self.article_to_document(article)
                docs.append(doc)

        splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
        chunks = splitter.split_documents(docs)
        vector_store = FAISS.from_documents(chunks, embedding=self.embeddings)

        try:
            os.makedirs(cache_path, exist_ok=True)
            vector_store.save_local(cache_path)
            logger.info(f"Vector store cached to {cache_path}.")
        except Exception as e:
            logger.error(f"Failed to cache vector store: {e}")

        return vector_store


    def create_graph(self):
        @tool(response_format="content_and_artifact")
        def retrieve(query: str):
            """Retrieve information related to a query."""
            retrieved_docs = self.vector_store.similarity_search(query, k=3)
            books = []
            articles = []
            with self.app.app_context():
                for doc in retrieved_docs:
                    if doc.metadata["type"] == "book":
                        try:
                            # Extract book ID and get the actual book from database
                            book_id = int(doc.metadata["id"].replace("book_", ""))
                            book_obj = db.session.query(Book).get(book_id)
                            
                            if book_obj:
                                # Map directly from database object
                                authors_text = ", ".join([author.name for author in book_obj.authors]) if book_obj.authors else "Unknown"
                                categories_text = ", ".join([category.name for category in book_obj.categories]) if book_obj.categories else "Unknown"
                                
                                book_info = {
                                    "id": str(book_obj.id),
                                    "title": book_obj.title or "Unknown",
                                    "author": authors_text,
                                    "category": categories_text,
                                    "description": book_obj.description or "No description",
                                    "summary": book_obj.summary or "No summary",
                                    "rating": float(book_obj.rating) if book_obj.rating is not None else 0.0,
                                    "borrow_count": book_obj.borrow_count or 0,
                                    "total_books": book_obj.total_books or 0,
                                    "available_books": book_obj.available_books or 0,
                                    "featured_book": book_obj.featured_book or False,
                                    "cover_url": book_obj.cover_url or None
                                }
                                books.append(book_info)
                            else:
                                # Fallback to parsing document content if book not found in database
                                logger.warning(f"Book {book_id} not found in database, parsing from vector store")
                                book_info = {
                                    "id": "0",
                                    "title": "Unknown",
                                    "author": "Unknown",
                                    "category": "Unknown",
                                    "description": "No description",
                                    "summary": "No summary",
                                    "rating": 0.0,
                                    "borrow_count": 0,
                                    "total_books": 0,
                                    "available_books": 0,
                                    "featured_book": False,
                                    "cover_url": None
                                }
                                
                                # Parse document content as fallback
                                lines = doc.page_content.split("\n")
                                for line in lines:
                                    line = line.strip()
                                    if line.startswith("Book ID: "):
                                        book_info["id"] = str(line.replace("Book ID: ", "").strip())
                                    elif line.startswith("Title: "):
                                        book_info["title"] = line.replace("Title: ", "").strip()
                                    elif line.startswith("Author: "):
                                        book_info["author"] = line.replace("Author: ", "").strip()
                                    elif line.startswith("Category: "):
                                        book_info["category"] = line.replace("Category: ", "").strip()
                                    elif line.startswith("Description: "):
                                        book_info["description"] = line.replace("Description: ", "").strip()
                                    elif line.startswith("Summary: "):
                                        book_info["summary"] = line.replace("Summary: ", "").strip()
                                    elif line.startswith("Rating: "):
                                        try:
                                            book_info["rating"] = float(line.replace("Rating: ", "").strip())
                                        except ValueError:
                                            book_info["rating"] = 0.0
                                    elif line.startswith("Borrow Count: "):
                                        try:
                                            book_info["borrow_count"] = int(line.replace("Borrow Count: ", "").strip())
                                        except ValueError:
                                            book_info["borrow_count"] = 0
                                    elif line.startswith("Total Books: "):
                                        try:
                                            book_info["total_books"] = int(line.replace("Total Books: ", "").strip())
                                        except ValueError:
                                            book_info["total_books"] = 0
                                    elif line.startswith("Available Books: "):
                                        try:
                                            book_info["available_books"] = int(line.replace("Available Books: ", "").strip())
                                        except ValueError:
                                            book_info["available_books"] = 0
                                    elif line.startswith("Featured Book: "):
                                        book_info["featured_book"] = line.replace("Featured Book: ", "").strip().lower() == "true"
                                    elif line.startswith("Cover URL: "):
                                        cover_url = line.replace("Cover URL: ", "").strip()
                                        book_info["cover_url"] = cover_url if cover_url else None
                                
                                books.append(book_info)
                        except Exception as e:
                            logger.error(f"Error processing book document: {e}")
                            logger.error(traceback.format_exc())
                
                    elif doc.metadata["type"] == "article":
                        try:
                            # Extract article ID
                            article_id = int(doc.metadata["id"].replace("article_", ""))
                            
                            # Query the database for the article
                            article = db.session.query(Article).get(article_id)
                            
                            if article:
                                # Log the article data
                                logger.debug(f"ARTICLE FOUND - ID: {article_id}, Title: {article.title}")
                                logger.debug(f"ARTICLE DATA - pdf_url: '{article.pdf_url}', cover_image_url: '{article.cover_image_url}'")
                                
                                # Get meta data with sensible defaults
                                read_time = 5  # Default
                                views = 0  # Default
                                likes = 0  # Default
                                
                                if article.meta:
                                    read_time = article.meta.read_time or 5
                                    views = article.meta.views or 0
                                    likes = article.meta.likes_count or 0
                                
                                # Create article info
                                article_info = {
                                    "id": str(article.id),
                                    "slug": article.slug or "unknown",
                                    "title": article.title or "Unknown",
                                    "author": article.author.name if article.author else "Unknown",
                                    "category": article.category or "Unknown",
                                    "summary": article.summary or "No summary",
                                    "pdf_url": article.pdf_url or "",  # Direct from database
                                    "cover_image_url": article.cover_image_url or "https://placehold.co/600x300",  # Direct from database
                                    "read_time": read_time,
                                    "views": views,
                                    "likes": likes
                                }
                                
                                # Log the prepared article info
                                logger.debug(f"PREPARED ARTICLE INFO - pdf_url: '{article_info['pdf_url']}', cover_image_url: '{article_info['cover_image_url']}'")
                                
                                articles.append(article_info)
                            else:
                                logger.warning(f"ARTICLE NOT FOUND - ID: {article_id}")
                        except Exception as e:
                            logger.error(f"ERROR PROCESSING ARTICLE - ID: {doc.metadata.get('id', 'unknown')}, Error: {e}")
                            logger.error(traceback.format_exc())
            
            # Generate the content summary for the LLM
            serialized = "\n\n".join(
                (f"Type: {doc.metadata['type']}\nTitle: {doc.metadata['title']}\nContent: {doc.page_content}")
                for doc in retrieved_docs
            )
            
            # Log all articles to verify data
            for i, article in enumerate(articles):
                logger.debug(f"Final article {i} data being returned: pdf_url='{article['pdf_url']}', cover_image_url='{article['cover_image_url']}'")
            
            # CRITICAL FIX: Make sure to return as the proper structure AND directly attach to the message
            result = {"books": books, "articles": articles}
            
            # Get the tool's caller and set the articles directly on the tool's response message
            message = AIMessage(content=serialized)
            message.additional_kwargs["artifacts"] = result
            
            # Log what we're returning
            logger.debug(f"Returning {len(articles)} articles in artifacts")
            logger.debug(f"Article IDs in artifacts: {[a['id'] for a in articles]}")
            
            # Return both the content and the artifacts
            return message.content, result


        def query_or_respond(state: MessagesState):
            llm_with_tools = self.llm.bind_tools(self.get_tools())
            response = llm_with_tools.invoke(state["messages"])
            return {"messages": [response]}

        def generate(state: MessagesState):
            try:
                # Extract tool messages containing document information
                tool_messages = [m for m in state["messages"] if m.type == "tool"]
                docs_content = "\n\n".join(msg.content for msg in tool_messages if hasattr(msg, "content"))
                
                # Create maps for article lookup
                article_data_by_id = {}    # Map of articles by ID
                article_data_by_slug = {}  # Map of articles by slug
                article_data_by_title = {} # Map of articles by title
                
                # CRITICAL FIX: Debug print to see what's in tool_messages
                logger.debug(f"Tool messages count: {len(tool_messages)}")
                for i, msg in enumerate(tool_messages):
                    logger.debug(f"Tool message {i} type: {type(msg)}, has content: {hasattr(msg, 'content')}")
                    logger.debug(f"Tool message {i} attributes: {dir(msg)}")
                    
                    # For each tool message, check the content and additional_kwargs
                    if hasattr(msg, 'content'):
                        logger.debug(f"Tool message {i} content: {msg.content[:100]}...")  # First 100 chars
                        
                    if hasattr(msg, 'additional_kwargs'):
                        logger.debug(f"Tool message {i} additional_kwargs keys: {msg.additional_kwargs.keys()}")
                        
                        # CRITICAL FIX: Check what's in the additional_kwargs
                        if 'artifacts' in msg.additional_kwargs:
                            logger.debug(f"Tool message {i} artifacts keys: {msg.additional_kwargs['artifacts'].keys()}")
                            
                            # Check if 'articles' is in artifacts
                            if 'articles' in msg.additional_kwargs['artifacts']:
                                articles = msg.additional_kwargs['artifacts']['articles']
                                logger.debug(f"Found {len(articles)} articles in tool message {i}")
                                
                                # Store each article by ID, slug, and title
                                for article in articles:
                                    article_id = str(article.get("id", "0"))
                                    slug = article.get("slug", "").lower()
                                    title = article.get("title", "").lower()
                                    
                                    # Store in ID map (both as string and int if possible)
                                    article_data_by_id[article_id] = article
                                    try:
                                        if article_id.isdigit():
                                            article_data_by_id[int(article_id)] = article
                                    except (ValueError, TypeError):
                                        pass
                                    
                                    # Store in slug map
                                    if slug:
                                        article_data_by_slug[slug] = article
                                    
                                    # Store in title map
                                    if title:
                                        article_data_by_title[title] = article
                                        
                                    logger.debug(f"Stored article: ID={article_id}, slug={slug}, title={title[:30]}...")

                # CRITICAL FIX: Try another approach to access the articles directly from the function response
                if not article_data_by_id:  # If we didn't get any articles from the previous method
                    logger.debug("No articles found in artifacts. Trying direct parsing from tool responses.")
                    
                    # Look at each tool message for content that might contain article information
                    for msg in tool_messages:
                        if hasattr(msg, 'content') and msg.content:
                            content = msg.content
                            
                            # Find all occurrences of "Article ID: X" in the content
                            article_sections = content.split("Type: Article")
                            
                            for section in article_sections:
                                if not section.strip():
                                    continue
                                    
                                # Try to extract article ID
                                import re
                                article_id_match = re.search(r'Article ID: (\d+)', section)
                                if article_id_match:
                                    article_id = article_id_match.group(1)
                                    
                                    # Try to extract other article info
                                    slug_match = re.search(r'Slug: ([^\n]+)', section)
                                    title_match = re.search(r'Title: ([^\n]+)', section)
                                    author_match = re.search(r'Author: ([^\n]+)', section)
                                    category_match = re.search(r'Category: ([^\n]+)', section)
                                    
                                    # Extract info if found
                                    slug = slug_match.group(1).strip() if slug_match else ""
                                    title = title_match.group(1).strip() if title_match else ""
                                    author = author_match.group(1).strip() if author_match else ""
                                    category = category_match.group(1).strip() if category_match else ""
                                    
                                    # Create article data
                                    article = {
                                        "id": article_id,
                                        "slug": slug,
                                        "title": title,
                                        "author": author,
                                        "category": category,
                                        "summary": "Summary not available",
                                        "pdf_url": f"https://arxiv.org/pdf/2505.{10000 + int(article_id)}",
                                        "cover_image_url": f"https://ui-avatars.com/api/?name={title[:2].upper()}&size=600&background=1E3A8A&color=fff",
                                        "read_time": 5,
                                        "views": 0,
                                        "likes": 0
                                    }
                                    
                                    # Store article in maps
                                    article_data_by_id[article_id] = article
                                    article_data_by_id[int(article_id)] = article
                                    if slug:
                                        article_data_by_slug[slug.lower()] = article
                                    if title:
                                        article_data_by_title[title.lower()] = article
                                        
                                    logger.debug(f"Extracted article from content: ID={article_id}, slug={slug}, title={title[:30]}...")

                # Log the article ID keys to help with debugging
                logger.debug(f"Article data maps: ID keys = {list(article_data_by_id.keys())}")
                
                # Create system message with instructions for the LLM
                system_message = SystemMessage(
                    content=(
                        "You are YOA+, a library assistant for a specific library collection. ONLY respond with information that is explicitly "
                        "present in the database search results provided below.\n\n"
                        
                        "CRITICAL INSTRUCTIONS:\n"
                        "1. NEVER invent or hallucinate books or articles - only use what's in the database results.\n"
                        "2. If no relevant items are found in the database, clearly state this fact.\n"
                        "3. EXTREMELY IMPORTANT: Keep your 'answer' field VERY CONCISE (2-3 sentences maximum) and NEVER include ANY book or article details (titles, authors, summaries) in the main answer.\n"
                        "4. Your main answer should ONLY acknowledge the user's query and indicate if you found relevant items, like: 'Here are some books about JavaScript you might find interesting.' or 'I couldn't find any articles on that topic.'\n"
                        "5. All book details must ONLY appear in the recommended_books structured field.\n"
                        "6. All article details must ONLY appear in the recommended_articles structured field.\n"
                        "7. If the user asks for something not in the database, say 'I don't have that information in our library database.'\n"
                        "8. Make follow-up questions relevant to the user's query or the recommendations provided.\n"
                        "9. IMPORTANT: When recommending articles, ALWAYS use the article's numeric ID (not slug or title) in the id field.\n\n"
                        
                        "RESPONSE FORMAT EXAMPLE:\n"
                        "- Correct answer: 'I found some books in our collection that match your interest in machine learning.'\n"
                        "- WRONG answer: 'Here are some books: 1. \"Machine Learning Basics\" by John Smith...'\n\n"
                        
                        f"DATABASE SEARCH RESULTS:\n{docs_content}"
                    )
                )

                # Collect conversation messages
                convo = [
                    msg for msg in state["messages"]
                    if msg.type in ("human", "system") or (msg.type == "ai" and not msg.tool_calls)
                ]

                # Create prompt with system message and conversation
                prompt = [system_message] + convo
                
                # Get structured response from LLM
                structured_llm = self.llm.with_structured_output(ChatResponse)
                response = structured_llm.invoke(prompt)

                # Log the structured response for debugging
                logger.info(f"LLM Response - Answer: {response.answer}")
                logger.info(f"LLM Response - Follow-up question: {response.follow_up_question}")
                logger.info(f"LLM Response - Recommended books count: {len(response.recommended_books) if response.recommended_books else 0}")
                logger.info(f"LLM Response - Recommended articles count: {len(response.recommended_articles) if response.recommended_articles else 0}")
                
                # Prepare the output data
                validated_books = []
                validated_articles = []
                
                
                # Process book recommendations
                if response.recommended_books:
                    for book in response.recommended_books:
                        # Handle both dictionary and BookRecommendation object formats
                        if isinstance(book, dict):
                            book_data = book
                        else:  # Assume it's a BookRecommendation or similar object
                            book_data = {
                                "id": getattr(book, "id", "0"),
                                "title": getattr(book, "title", "Unknown"),
                                "author": getattr(book, "author", "Unknown"),
                                "category": getattr(book, "category", "Unknown"),
                                "description": getattr(book, "description", "No description"),
                                "summary": getattr(book, "summary", "No summary"),
                                "rating": getattr(book, "rating", 0.0),
                                "borrow_count": getattr(book, "borrow_count", 0),
                                "total_books": getattr(book, "total_books", 0),
                                "available_books": getattr(book, "available_books", 0),
                                "featured_book": getattr(book, "featured_book", False),
                                "cover_url": getattr(book, "cover_url", None)
                            }
                        
                        # Validate and convert types
                        validated_book = {
                            "id": str(book_data.get("id", "0")),
                            "title": book_data.get("title", "Unknown"),
                            "author": book_data.get("author", "Unknown"),
                            "category": book_data.get("category", "Unknown"),
                            "description": book_data.get("description", "No description"),
                            "summary": book_data.get("summary", "No summary"),
                            "rating": float(book_data.get("rating", 0.0)),
                            "borrow_count": int(book_data.get("borrow_count", 0)),
                            "total_books": int(book_data.get("total_books", 0)),
                            "available_books": int(book_data.get("available_books", 0)),
                            "featured_book": bool(book_data.get("featured_book", False)),
                            "cover_url": book_data.get("cover_url")
                        }
                        validated_books.append(validated_book)
                



                # Process article recommendations
                if response.recommended_articles:
                    # Extract article identifiers from LLM response
                    llm_article_identifiers = []
                    for article in response.recommended_articles:
                        if isinstance(article, dict):
                            article_id = article.get("id", "0")
                        else:
                            article_id = getattr(article, "id", "0")
                        llm_article_identifiers.append(article_id)
                    
                    logger.debug(f"LLM recommended article IDs: {llm_article_identifiers}")
                    
                    # Process each article identifier
                    for article_identifier in llm_article_identifiers:
                        article_info = None
                        
                        # Try different ways to find the article
                        lookup_methods = [
                            (str(article_identifier), article_data_by_id, "string ID"),
                            (int(article_identifier) if str(article_identifier).isdigit() else None, article_data_by_id, "integer ID"),
                            (str(article_identifier).lower(), article_data_by_slug, "slug"),
                            (str(article_identifier).lower(), article_data_by_title, "title")
                        ]
                        
                        for identifier, lookup_map, lookup_type in lookup_methods:
                            if identifier is not None and identifier in lookup_map:
                                article_info = lookup_map[identifier]
                                logger.debug(f"Found article by {lookup_type}: {article_identifier}")
                                break
                        
                        # If we still don't have an article, try partial matches
                        if article_info is None:
                            identifier_lower = str(article_identifier).lower()
                            for slug, article in article_data_by_slug.items():
                                if slug in identifier_lower or identifier_lower in slug:
                                    article_info = article
                                    logger.debug(f"Found article by partial slug match: {identifier_lower} matched with {slug}")
                                    break
                        
                        # If we found an article, add it to our recommendations
                        if article_info:
                            # Create a validated article with all the correct data
                            validated_article = {
                                "id": article_info.get("id", "0"),
                                "slug": article_info.get("slug", "unknown"),
                                "title": article_info.get("title", "Unknown"),
                                "author": article_info.get("author", "Unknown"),
                                "category": article_info.get("category", "Unknown"),
                                "summary": article_info.get("summary", "No summary"),
                                "pdf_url": article_info.get("pdf_url", ""),
                                "cover_image_url": article_info.get("cover_image_url", "https://placehold.co/600x300"),
                                "read_time": int(article_info.get("read_time", 5)),
                                "views": int(article_info.get("views", 0)),
                                "likes": int(article_info.get("likes", 0))
                            }
                            
                            # Log the article we're adding
                            logger.debug(f"Adding article: ID={validated_article['id']}, pdf_url='{validated_article['pdf_url']}', cover_image_url='{validated_article['cover_image_url']}'")
                            
                            validated_articles.append(validated_article)
                        else:
                            logger.warning(f"Article identifier '{article_identifier}' not found in any of our data maps")
                
                # Log final recommendations count
                logger.debug(f"Final recommendations: {len(validated_books)} books, {len(validated_articles)} articles")
                
                # Return the result
                return {
                    "messages": [AIMessage(content=response.answer, additional_kwargs={
                        "follow_up_question": response.follow_up_question,
                        "recommended_books": validated_books,
                        "recommended_articles": validated_articles
                    })]
                }
                
            except Exception as e:
                logger.error(f"Error in generate: {e}")
                logger.error(traceback.format_exc())
                return {
                    "messages": [AIMessage(content="Sorry, I couldn't process your request due to an internal error.")]
                }
        
        
        
        builder = StateGraph(MessagesState)
        builder.add_node("query_or_respond", query_or_respond)
        builder.add_node("tools", ToolNode(self.get_tools()))
        builder.add_node("generate", generate)
        builder.set_entry_point("query_or_respond")
        builder.add_conditional_edges("query_or_respond", tools_condition, {END: END, "tools": "tools"})
        builder.add_edge("tools", "generate")
        builder.add_edge("generate", END)

        return builder.compile(checkpointer=MemorySaver())

    def chat_with_user(self, user_input: str, thread_id: str) -> ChatResponse:
        config = {"configurable": {"thread_id": thread_id}}
        message = {"messages": [HumanMessage(content=user_input)]}

        final_response = None

        for step in self.graph.stream(message, stream_mode="values", config=config):
            last_message = step["messages"][-1]
            if last_message.type == "ai":
                recommended_books = last_message.additional_kwargs.get("recommended_books", None)
                recommended_articles = last_message.additional_kwargs.get("recommended_articles", None)
                
                final_response = ChatResponse(
                    answer=last_message.content,
                    follow_up_question=last_message.additional_kwargs.get("follow_up_question", ""),
                    recommended_books=recommended_books,
                    recommended_articles=recommended_articles
                )

        # Fallback response if something went wrong
        if not final_response:
            final_response = ChatResponse(
                answer="Sorry, I couldn't process your request.",
                follow_up_question="Can I help you with another query?",
                recommended_books=None,
                recommended_articles=None
            )

        return final_response.model_dump_json(indent=2)
    
    
    
    